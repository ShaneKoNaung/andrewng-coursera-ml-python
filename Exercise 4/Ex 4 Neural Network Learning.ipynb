{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images : (5000, 400)\n",
      "Training labels : (5000,)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "X = data['X']\n",
    "y = data['y'].ravel()\n",
    "\n",
    "m = len(y)\n",
    "\n",
    "print(f'Training images : {X.shape}')\n",
    "print(f'Training labels : {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta1 : (25, 401)\n",
      "Theta2 : (10, 26)\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "\n",
    "weights = loadmat('ex4weights.mat')\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "print(f'Theta1 : {Theta1.shape}')\n",
    "print(f'Theta2 : {Theta2.shape}')\n",
    "\n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost (Feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "    \n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    A1 = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    Z2 = A1.dot(Theta1.T)\n",
    "    \n",
    "    A2 = sigmoid(Z2)\n",
    "    A2 = np.append(np.ones((len(A2),1)), A2, axis=1)\n",
    "    \n",
    "    Z3 = A2.dot(Theta2.T)\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    J = 0\n",
    "    \n",
    "    # convert y into len(y) x num_labels matrix\n",
    "    new_y = np.zeros((len(y), num_labels))\n",
    "    \n",
    "    for i in range(m):\n",
    "        new_y[i][y[i] - 1] = 1   # new_y would have [0 to 9] instead of y [1 to 10]\n",
    "    \n",
    "    inner_sum  =0\n",
    "    \n",
    "    for i in range(num_labels):\n",
    "        inner_sum += - (new_y[:,i].T.dot(np.log(A3[:,i])) + (1 - new_y[:,i]).T.dot(np.log(1 - A3[:,i])))\n",
    "    \n",
    "    J = (1/m)  * inner_sum\n",
    "    \n",
    "    # cost function with regularization\n",
    "\n",
    "    J +=  (lmbda/(2 * m)) * (np.sum(np.sum((Theta1[:,1:] ** 2), axis=1)) + np.sum(np.sum((Theta2[:,1:] ** 2), axis=1)))\n",
    "    \n",
    "    X = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    \n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a1 = X[i, :].T    # 401x1\n",
    "        a1 = a1.reshape(len(a1),1)\n",
    "        # ====== Forward Propagation =======\n",
    "        \n",
    "        z2 = Theta1.dot(a1)  # 25x401 * 401x1\n",
    "        a2 = sigmoid(z2)  # 25x1\n",
    "        \n",
    "        a2 = np.append(np.ones((1,a2.shape[1])), a2, axis=0)  # 26x1\n",
    "        \n",
    "        z3 = Theta2.dot(a2)  #10x26 * 26x1\n",
    "        a3 = sigmoid(z3)\n",
    "        \n",
    "        # Compute delta\n",
    "        \n",
    "        delta3 = a3 - new_y[i, :].T.reshape(len(new_y[i,:]),1)\n",
    "        delta2 = (Theta2.T.dot(delta3)) * a2 * (1 - a2)\n",
    "    \n",
    "        Theta1_grad += delta2[1:,:].dot(a1.T)\n",
    "        \n",
    "        Theta2_grad += delta3.dot(a2.T)\n",
    "        \n",
    "    Theta1_grad = (1/m) * Theta1_grad\n",
    "    Theta2_grad *= (1/m)\n",
    "    \n",
    "    Theta1_grad[:, 1:] += (lmbda/m) * Theta1_grad[:, 1:]\n",
    "    Theta2_grad[:, 1:] += (lmbda/m) * Theta2_grad[:, 1:]\n",
    "    \n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "        \n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward Using Neural Networks ... \n",
      "\n",
      "Cost at parameters (loaded from ex4weights): 0.287629165161319 \n",
      "(this value should be about 0.287629)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Feedforward Using Neural Networks ... \\n')\n",
    "\n",
    "lmbda = 0\n",
    "\n",
    "J, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)\n",
    "\n",
    "print(f'Cost at parameters (loaded from ex4weights): {J} \\n(this value should be about 0.287629)\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Cost Function (w/ Regularization) ... \n",
      "\n",
      "Cost at parameters (loaded from ex4weights): 0.38376985909092376 \n",
      "(this value should be about 0.383770)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
    "\n",
    "# Weight regularization parameter (we set this to 1 here).\n",
    "lmbda = 1\n",
    "\n",
    "J, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)\n",
    "\n",
    "print(f'Cost at parameters (loaded from ex4weights): {J} \\n(this value should be about 0.383770)\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating sigmoid gradient...\n",
      "\n",
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[[0.19661193 0.23500371 0.25       0.23500371 0.19661193]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nEvaluating sigmoid gradient...\\n')\n",
    "\n",
    "g = sigmoidGradient(np.array([[-1, -0.5, 0, 0.5, 1]]));\n",
    "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ');\n",
    "print(g);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Pameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    W = np.zeros((L_in, L_out + 1))\n",
    "    \n",
    "   # One effective strategy for random initialization is to randomly select values for Θ (l) uniformly in the range [−\u000f init , \u000f init ].\n",
    "   # You should use \u000f init = 0.12. 2 This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "    epsilon_init = 0.12\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Neural Network Parameters ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nInitializing Neural Network Parameters ...\\n')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel() , initial_Theta2.ravel()]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Backpropagation and Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training Neural Network ... \\n')\n",
    "\n",
    "lmbda = 1\n",
    "\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lmbda)\n",
    "\n",
    "\n",
    "res = minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options={'maxiter': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = res.x\n",
    "\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    \n",
    "    X = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    h1 = sigmoid(X.dot(Theta1.T))\n",
    "    \n",
    "    h1 = np.append(np.ones((len(h1),1)), h1, axis=1)\n",
    "    h2 = sigmoid(h1.dot(Theta2.T))\n",
    "    \n",
    "    p = np.argmax(h2, axis=1)\n",
    "    \n",
    "    return p + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy : 96.02000000000001 %\n"
     ]
    }
   ],
   "source": [
    "p = predict(Theta1, Theta2, X)\n",
    "print(f'Training set accuracy : {np.mean(p == y.reshape(len(y))) * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
